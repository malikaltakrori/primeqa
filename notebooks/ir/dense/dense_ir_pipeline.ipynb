{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e95342",
   "metadata": {},
   "source": [
    "# Dense IR using _Pipelines_ interface\n",
    "\n",
    "In this notebook, we show how to index data, and run search using the _Pipelines_ API.\n",
    "In orded to run (almost) instantaneously, we use trivial data sizes of training data and collection to search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51992c4b",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "We start by defining variables specifying locations of data we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0c7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "model_name_or_path = \"PrimeQA/DrDecr_XOR-TyDi_whitebox-2\"\n",
    "test_files_location = '../../../tests/resources/ir_dense'\n",
    "with tempfile.TemporaryDirectory() as working_dir:\n",
    "    output_dir=os.path.join(working_dir, 'output_dir')\n",
    "    \n",
    "index_root = os.path.join(output_dir, 'index_root')\n",
    "index_name = 'index_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163b958",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "To run indexing, we need an existing model (checkpoint). For this tutorial, we will use the [Dr.Decr](https://huggingface.co/PrimeQA/DrDecr_XOR-TyDi_whitebox-2) model from huggingface.\n",
    "Next, we will index a collection of documents, using model representaion from the previous step. The collection is a TSV file, containing each document's ID, title, and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62caf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_fn = os.path.join(test_files_location, \"xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b0e3c",
   "metadata": {},
   "source": [
    "Here is an example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdb8743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"de facto\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.</td>\n",
       "      <td>Kangxi Emperor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "data = pd.read_csv(collection_fn, sep='\\t', header=0, nrows=1)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db322714",
   "metadata": {},
   "source": [
    "Next we instantiate the indexer and index the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90784083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.8'\n",
      "{\"time\":\"2023-11-03 04:28:54,917\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-11-03 04:28:54,980\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n",
      "[Nov 03, 04:28:55] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:28:55] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2/artifact.metadata\n",
      "\n",
      "\n",
      "[Nov 03, 04:28:55] #> Creating directory /tmp/tmpvtm_ip29/output_dir/index_root/index_name \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.8'\n",
      "{\"time\":\"2023-11-03 04:29:22,528\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-11-03 04:29:22,557\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": \"\\/tmp\\/tmpvtm_ip29\\/output_dir\\/index_root\\/index_name\",\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"num_partitions_max\": 2,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"PrimeQA\\/DrDecr_XOR-TyDi_whitebox-2\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/dccstor\\/cssblr\\/rbhat\\/projects\\/gitrepos\\/primeqa\\/notebooks\\/ir\\/dense\\/experiments\",\n",
      "    \"experiment\": \"default\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-11\\/03\\/04.28.22\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Nov 03, 04:29:22] #> Loading collection...\n",
      "0M \n",
      "[Nov 03, 04:29:22] #>>>>> at ColBERT name (model name) : PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:29:22] #>>>>> at BaseColBERT name (model name) : PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:29:22] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:29:22] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2/artifact.metadata\n",
      "[Nov 03, 04:29:22] factory model type: xlm-roberta\n",
      "[Nov 03, 04:29:33] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Nov 03, 04:29:34] factory model type: xlm-roberta\n",
      "[Nov 03, 04:29:34] factory model type: xlm-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cssblr/rbhat/.envs/pqa/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/dccstor/cssblr/rbhat/.envs/pqa/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 03, 04:29:35] [0] \t\t # of sampled PIDs = 7 \t sampled_pids[:3] = [3, 5, 0]\n",
      "[Nov 03, 04:29:35] [0] \t\t #> Encoding 7 passages..\n",
      "[Nov 03, 04:29:35] #> checkpoint, docFromText, Input: title | text, \t\t 64\n",
      "[Nov 03, 04:29:35] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Nov 03, 04:29:35] #> Input: $ title | text, \t\t 64\n",
      "[Nov 03, 04:29:35] #> Output IDs: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Nov 03, 04:29:35] #> Output Mask: torch.Size([180]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Nov 03, 04:29:35] #> checkpoint, docFromText, Output IDs: (tensor([[     0,   9749,  44759,  ...,      1,      1,      1],\n",
      "        [     0,   9749,  30267,  ...,      1,      1,      1],\n",
      "        [     0,   9749,  31678,  ...,      5,      2,      1],\n",
      "        ...,\n",
      "        [     0,   9749,   9098,  ...,      1,      1,      1],\n",
      "        [     0,   9749,    341,  ..., 120025,     92,      2],\n",
      "        [     0,   9749,  11617,  ...,      1,      1,      1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[Nov 03, 04:29:35] #>>>> colbert doc ==\n",
      "[Nov 03, 04:29:35] #>>>>> input_ids: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Nov 03, 04:29:37] #>>>> before linear doc ==\n",
      "[Nov 03, 04:29:37] #>>>>> D: torch.Size([180, 768]), tensor([[-0.0607,  0.2010,  0.1768,  ..., -0.5506, -0.2222,  0.2935],\n",
      "        [-0.1522,  0.0394,  0.0804,  ..., -0.1502, -0.0579,  0.3067],\n",
      "        [-0.0690, -0.1527, -0.2328,  ..., -0.2470,  0.0126,  0.1894],\n",
      "        ...,\n",
      "        [-0.2182, -0.1169, -0.3343,  ..., -0.6815,  0.0170,  0.7173],\n",
      "        [-0.2182, -0.1169, -0.3343,  ..., -0.6815,  0.0170,  0.7173],\n",
      "        [-0.2182, -0.1169, -0.3343,  ..., -0.6815,  0.0170,  0.7173]])\n",
      "[Nov 03, 04:29:37] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-0.0286,  0.0017, -0.0202,  ..., -0.0262,  0.0210,  0.0006],\n",
      "        [-0.0102,  0.0121, -0.0111,  ..., -0.0362, -0.0165, -0.0012],\n",
      "        [-0.0047, -0.0172, -0.0054,  ..., -0.0069, -0.0194, -0.0193],\n",
      "        ...,\n",
      "        [-0.0286,  0.0231,  0.0004,  ...,  0.0373, -0.0045,  0.0125],\n",
      "        [ 0.0051,  0.0023,  0.0212,  ..., -0.0254,  0.0034,  0.0206],\n",
      "        [-0.0068,  0.0256, -0.0263,  ...,  0.0200,  0.0125, -0.0149]],\n",
      "       requires_grad=True)\n",
      "[Nov 03, 04:29:37] #>>>> colbert doc ==\n",
      "[Nov 03, 04:29:37] #>>>>> D: torch.Size([180, 128]), tensor([[ 0.3245,  0.1604,  0.7869,  ...,  0.1677,  0.6785, -0.4504],\n",
      "        [ 0.5325,  0.5930,  1.9998,  ...,  0.3003,  0.8591, -1.2359],\n",
      "        [ 0.4889, -0.1628,  1.9015,  ...,  0.2000,  1.2535, -1.0218],\n",
      "        ...,\n",
      "        [ 0.4503,  0.1676,  2.3307,  ...,  0.4687,  0.8542, -1.3855],\n",
      "        [ 0.4503,  0.1676,  2.3307,  ...,  0.4687,  0.8542, -1.3855],\n",
      "        [ 0.4503,  0.1676,  2.3307,  ...,  0.4687,  0.8542, -1.3855]])\n",
      "[Nov 03, 04:29:37] [0] \t\t avg_doclen_est = 174.57142639160156 \t len(local_sample) = 7\n",
      "[Nov 03, 04:29:37] >> num_partitions_multiplier = 8, self.num_partitions = 256\n",
      "[Nov 03, 04:29:37] >> num_partitions limited to: self.num_partitions = 2\n",
      "[Nov 03, 04:29:37] [0] \t\t Creaing 2 partitions.\n",
      "[Nov 03, 04:29:37] [0] \t\t *Estimated* 1,221 embeddings.\n",
      "[Nov 03, 04:29:37] [0] \t\t #> Saving the indexing plan to /tmp/tmpvtm_ip29/output_dir/index_root/index_name/plan.json ..\n",
      "Sampling a subset of 512 / 1161 for training\n",
      "Clustering 512 points in 128D to 2 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[0.053, 0.066, 0.041, 0.044, 0.033, 0.043, 0.056, 0.038, 0.06, 0.058, 0.051, 0.041, 0.033, 0.055, 0.046, 0.038, 0.042, 0.034, 0.038, 0.047, 0.055, 0.043, 0.052, 0.039, 0.034, 0.037, 0.035, 0.044, 0.057, 0.05, 0.065, 0.039, 0.043, 0.04, 0.042, 0.056, 0.031, 0.061, 0.052, 0.041, 0.052, 0.051, 0.042, 0.043, 0.06, 0.059, 0.05, 0.044, 0.045, 0.037, 0.036, 0.044, 0.072, 0.049, 0.058, 0.039, 0.04, 0.034, 0.047, 0.056, 0.05, 0.043, 0.046, 0.055, 0.037, 0.061, 0.049, 0.048, 0.048, 0.053, 0.051, 0.041, 0.05, 0.06, 0.049, 0.045, 0.068, 0.053, 0.072, 0.042, 0.062, 0.036, 0.039, 0.026, 0.033, 0.054, 0.05, 0.037, 0.027, 0.057, 0.043, 0.041, 0.069, 0.038, 0.042, 0.034, 0.057, 0.065, 0.041, 0.036, 0.052, 0.04, 0.059, 0.048, 0.047, 0.05, 0.054, 0.063, 0.044, 0.049, 0.036, 0.065, 0.068, 0.041, 0.044, 0.069, 0.06, 0.045, 0.049, 0.039, 0.043, 0.059, 0.052, 0.056, 0.051, 0.064, 0.047, 0.042]\n",
      "[Nov 03, 04:29:37] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[Nov 03, 04:29:37] #> Got bucket_cutoffs = tensor([0.0001]) and bucket_weights = tensor([-0.0351,  0.0397])\n",
      "[Nov 03, 04:29:37] avg_residual = 0.047912076115608215\n",
      "[Nov 03, 04:29:37] #> base_config.py from_path /tmp/tmpvtm_ip29/output_dir/index_root/index_name/metadata.json\n",
      "[Nov 03, 04:29:37] #> base_config.py from_path /tmp/tmpvtm_ip29/output_dir/index_root/index_name/plan.json\n",
      "[Nov 03, 04:29:37] #> base_config.py from_path args loaded! \n",
      "[Nov 03, 04:29:37] #> base_config.py from_path args replaced ! \n",
      "[Nov 03, 04:29:37] [0] \t\t #> Encoding 7 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 03, 04:29:38] [0] \t\t #> Saving chunk 0: \t 7 passages and 1,222 embeddings. From #0 onward.\n",
      "[Nov 03, 04:29:38] offset: 0\n",
      "[Nov 03, 04:29:38] chunk codes size(0): 1222\n",
      "[Nov 03, 04:29:38] codes size(0): 1222\n",
      "[Nov 03, 04:29:38] codes size(): torch.Size([1222])\n",
      "[Nov 03, 04:29:38] >>>>partition.size(0): 2\n",
      "[Nov 03, 04:29:38] >>>>num_partition: 2\n",
      "[Nov 03, 04:29:38] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Nov 03, 04:29:38] #> Building the emb2pid mapping..\n",
      "[Nov 03, 04:29:38] len(emb2pid) = 1222\n",
      "[Nov 03, 04:29:38] #> Saved optimized IVF to /tmp/tmpvtm_ip29/output_dir/index_root/index_name/ivf.pid.pt\n",
      "[Nov 03, 04:29:38] [0] \t\t #> Saving the indexing metadata to /tmp/tmpvtm_ip29/output_dir/index_root/index_name/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.10s/it]\r",
      "1it [00:01,  1.11s/it]\n",
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]\r",
      "100%|██████████| 2/2 [00:00<00:00, 15679.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.indexer.dense import ColBERTIndexer\n",
    "os.makedirs(index_root, exist_ok = True)\n",
    "#checkpoint_fn = os.path.join(test_files_location, \"DrDecr.dnn\")\n",
    "\n",
    "indexer = ColBERTIndexer(checkpoint = model_name_or_path, index_root = index_root, index_name = index_name, num_partitions_max = 2)\n",
    "indexer.load()\n",
    "indexer.index(collection = collection_fn, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4033c",
   "metadata": {},
   "source": [
    "### Search\n",
    "Next, we use the trained model and the index to search the collection, using queries in the form of a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e919ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 03, 04:30:28] #> base_config.py from_path /tmp/tmpvtm_ip29/output_dir/index_root/index_name/metadata.json\n",
      "[Nov 03, 04:30:28] #> base_config.py from_path args loaded! \n",
      "[Nov 03, 04:30:28] #> base_config.py from_path args replaced ! \n",
      "[Nov 03, 04:30:28] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:30:28] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2/artifact.metadata\n",
      "[Nov 03, 04:30:28] #>>>>> at ColBERT name (model name) : PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:30:28] #>>>>> at BaseColBERT name (model name) : PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:30:28] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2\n",
      "[Nov 03, 04:30:28] #> base_config.py load_from_checkpoint PrimeQA/DrDecr_XOR-TyDi_whitebox-2/artifact.metadata\n",
      "[Nov 03, 04:30:28] factory model type: xlm-roberta\n",
      "[Nov 03, 04:30:38] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Nov 03, 04:30:38] factory model type: xlm-roberta\n",
      "[Nov 03, 04:30:39] factory model type: xlm-roberta\n",
      "[Nov 03, 04:30:40] #> Loading codec...\n",
      "[Nov 03, 04:30:40] #> base_config.py from_path /tmp/tmpvtm_ip29/output_dir/index_root/index_name/metadata.json\n",
      "[Nov 03, 04:30:40] #> base_config.py from_path args loaded! \n",
      "[Nov 03, 04:30:40] #> base_config.py from_path args replaced ! \n",
      "[Nov 03, 04:30:40] #> Loading IVF...\n",
      "[Nov 03, 04:30:40] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cssblr/rbhat/.envs/pqa/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 03, 04:30:40] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Nov 03, 04:30:41] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Nov 03, 04:30:41] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Nov 03, 04:30:41] #> Input: $ Who is Michael Wigge, \t\t True, \t\t None\n",
      "[Nov 03, 04:30:41] #> Output IDs: torch.Size([32]), tensor([    0,  9748, 40469,    83, 11617,  5140, 23359,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Nov 03, 04:30:41] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Nov 03, 04:30:41] #>>>> colbert query ==\n",
      "[Nov 03, 04:30:41] #>>>>> input_ids: torch.Size([32]), tensor([    0,  9748, 40469,    83, 11617,  5140, 23359,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cssblr/rbhat/.envs/pqa/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nov 03, 04:30:42] #>>>> before linear query ==\n",
      "[Nov 03, 04:30:42] #>>>>> Q: torch.Size([32, 768]), tensor([[-0.0056,  0.1896, -0.2805,  ..., -0.5975, -0.3414,  0.4975],\n",
      "        [-0.5635,  0.2752, -0.2151,  ..., -0.9036,  0.3531,  0.5631],\n",
      "        [-0.3071,  0.2370, -0.1805,  ..., -0.5553,  0.3696,  0.3634],\n",
      "        ...,\n",
      "        [-0.1919,  0.1687, -0.2729,  ..., -0.7984, -0.0091,  0.5765],\n",
      "        [-0.1919,  0.1687, -0.2729,  ..., -0.7984, -0.0091,  0.5765],\n",
      "        [-0.1919,  0.1687, -0.2729,  ..., -0.7984, -0.0091,  0.5765]])\n",
      "[Nov 03, 04:30:42] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-0.0286,  0.0017, -0.0202,  ..., -0.0262,  0.0210,  0.0006],\n",
      "        [-0.0102,  0.0121, -0.0111,  ..., -0.0362, -0.0165, -0.0012],\n",
      "        [-0.0047, -0.0172, -0.0054,  ..., -0.0069, -0.0194, -0.0193],\n",
      "        ...,\n",
      "        [-0.0286,  0.0231,  0.0004,  ...,  0.0373, -0.0045,  0.0125],\n",
      "        [ 0.0051,  0.0023,  0.0212,  ..., -0.0254,  0.0034,  0.0206],\n",
      "        [-0.0068,  0.0256, -0.0263,  ...,  0.0200,  0.0125, -0.0149]],\n",
      "       requires_grad=True)\n",
      "[Nov 03, 04:30:42] #>>>> colbert query ==\n",
      "[Nov 03, 04:30:42] #>>>>> Q: torch.Size([32, 128]), tensor([[ 0.5975,  0.0965, -0.1314,  ...,  0.6043,  0.5532, -0.4358],\n",
      "        [ 1.1322, -0.0928, -0.2966,  ...,  1.0765,  0.3959, -0.1884],\n",
      "        [ 0.9083, -0.3459, -0.0414,  ...,  1.2060,  0.4018, -0.4082],\n",
      "        ...,\n",
      "        [ 1.1509,  0.7642, -0.1032,  ...,  0.4308,  0.8157, -0.5313],\n",
      "        [ 1.1509,  0.7642, -0.1032,  ...,  0.4308,  0.8157, -0.5313],\n",
      "        [ 1.1509,  0.7642, -0.1032,  ...,  0.4308,  0.8157, -0.5313]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 84.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.retriever.dense import ColBERTRetriever\n",
    "\n",
    "retriever = ColBERTRetriever(checkpoint = model_name_or_path, index_root = index_root, index_name = index_name, ndocs = 5, max_num_documents = 2)\n",
    "retriever.load()\n",
    "results = retriever.predict(input_texts = ['Who is Michael Wigge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc50fb",
   "metadata": {},
   "source": [
    "Here is the top search result for our query, containing document_id and score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81d5cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 18.614036560058594)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c6478",
   "metadata": {},
   "source": [
    "Here is the top retrieved document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff3e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\tMichael Wigge Michael Wigge is a travel writer and entertainment personality in Europe and in the United States. His work is characterized by a mixture of journalism and entertainment. His specialties are cultural issues which he examines in a very entertaining way. In 2002, Wigge drew attention to himself in Germany for the first time on TV broadcaster VIVA plus presenting comedy clips on the daily show “London Calling”. In this context he sets a record for the longest donkey ride in music television history and visits the Queen of England, dressed as King Henry VIII, on her 50th throne\tMichael Wigge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(collection_fn, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if str(results[0][0][0]) == line.split()[0]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf5a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1ab85b37f5f7642ecf51c394f42be1f6421955e34dbae8f37bc531712479088"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
